# Character Level LSTM - Tensorflow

I trained a multi-layer cahracter level LSTM with the following cell archictecture.

![LSTM](https://i.imgur.com/gxBbaX2.png)

The network has three stacked layers of cells with a batch sequence length size of 100. I trained on an single NVIDIA GeForce 1080.

Random weights:

>U—CN3gLBddDWÁu!HÁ”;ap/8!ôdPEç’hÁóblOxD/ç)Vsn@i)Á”YáPxwDöAhDœ “uwDóJ$—u)cDèBDü->èAàZ@zàB(ióH$äaFu3ýYmFniRLMÉKmYwBwZO@Uuaîq@AG()%aèLî$5b%üpïER=—‘À*mê?ç:V
>v
>B’BlcFIy5hf98JouV﻿ jAúoöt(y(WWnRE;él:xGlêEWÀnoG



Here are some examples of text halucinations:

>“What would you have, my dear fellow!” answered Pierre, shrugging
>his shoulders. “Women, my dear fellow; women!”
>
>“I don’t understand it,” replied Prince Andrew. “Women who are

>t on the torders had an op wear still .natos the count’s fifteen-year-old niece, and little Pétya,
>his youngest boy, had all settled down in the drawing room and weariny before himself on his ket, fro

>ther could in the duarks at on the colpres. Anla me all stining with the rustling of dresses and
>the scraping of chairs. Then one of those conversations began which last
>out until, at the first pause,
